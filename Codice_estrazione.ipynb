{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TELEGRAM_BOT_TOKEN = ''\n",
    "TELEGRAM_CHAT_ID = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v_7wl3GUJKPC"
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "import pandas as pd\n",
    "import os\n",
    "import tarfile \n",
    "import zipfile\n",
    "import tensorflow as tf\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UMk2c6LMCPap"
   },
   "outputs": [],
   "source": [
    "base_dir = '.'\n",
    "dataset_folder = os.path.join(base_dir,'license-blobs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import shutil\n",
    "\n",
    "def gunzip_shutil(source_filepath, dest_filepath, block_size=65536):\n",
    "    with gzip.open(source_filepath, 'rb') as s_file, \\\n",
    "            open(dest_filepath, 'wb') as d_file:\n",
    "        shutil.copyfileobj(s_file, d_file, block_size)\n",
    "\n",
    "def is_gz_file(filepath):\n",
    "    with open(filepath, 'rb') as test_f:\n",
    "        return test_f.read(2) == b'\\x1f\\x8b'\n",
    "\n",
    "# try if everything is ok\n",
    "filename1 = os.path.join(base_dir, \"license-blobs/00/0a/000a799c27dfbad9a943dbf585d00f0cbb81a32c.tmp.tmp\")\n",
    "if os.path.exists(filename1) and is_gz_file(filename1):\n",
    "    gunzip_shutil(filename1, filename1+\".tmp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install charset_normalizer\n",
    "from charset_normalizer import detect\n",
    "\n",
    "def get_encoding_filename(filename):\n",
    "    try:\n",
    "        return detect(open(filename, 'rb').read()).get('encoding')\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_file = 0\n",
    "from collections import Counter\n",
    "cnt = Counter()\n",
    "def extract(corpus_path):\n",
    "    global num_file\n",
    "    corpus_path, dirs, files = next(os.walk(corpus_path))\n",
    "\n",
    "    corpus_path, dirs, files1 = next(os.walk(corpus_path))\n",
    "    for D1 in dirs:\n",
    "       \n",
    "        dir_path = os.sep.join([corpus_path,D1])\n",
    "        print(f\"{time.strftime('%H:%M:%S')} Current path: {dir_path}\")\n",
    "        dir_path, dirs2, files2 = next(os.walk(dir_path))\n",
    "        for D2 in dirs2:\n",
    "          \n",
    "          dir_path2 = os.sep.join([dir_path, D2])\n",
    "          print(f\"{time.strftime('%H:%M:%S')} Current folder: {dir_path2}\")\n",
    "          dir_path2, dirs3, files3 = next(os.walk(dir_path2))\n",
    "\n",
    "          for f in files3:\n",
    "            try:\n",
    "              data_path = os.sep.join([dir_path2, f])\n",
    "              \n",
    "              # extract file\n",
    "              if is_gz_file(data_path):\n",
    "                    gunzip_shutil(data_path, data_path+\".tmp\")\n",
    "                    # remove old file\n",
    "                    os.remove(data_path)\n",
    "                    data_path += \".tmp\"\n",
    "              num_file += 1\n",
    "              cnt[get_encoding_filename(data_path)] += 1\n",
    "            except Exception as e:\n",
    "              print(e)\n",
    "        print(f\"{time.strftime('%H:%M:%S')} Numero di file letti: {num_file}\")\n",
    "        print(f\"{time.strftime('%H:%M:%S')} {cnt.most_common(10)}\")\n",
    "extract(dataset_folder)\n",
    "print(f\"{time.strftime('%H:%M:%S')} {cnt.most_common(10)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Download dataset and extract\n",
    "if not os.path.exists(os.path.abspath('.') + dataset_folder):\n",
    "  dataset_tar = tf.keras.utils.get_file('license-blobs.tar',\n",
    "                                      cache_subdir=os.path.abspath('.')+dataset_folder,\n",
    "                                      origin = 'https://casacalarota.onthewifi.com/wp-content/uploads/2020/10/RDC/license-blobs.tar',\n",
    "                                      extract = True)\n",
    "  PATH = os.path.dirname(dataset_tar) + dataset_folder\n",
    "  os.remove(dataset_tar)\n",
    "  extract(dataset_folder)\n",
    "else:\n",
    "  PATH = os.path.abspath('.') + dataset_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iY1zJ3V5UAZ9"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IY_QwrU6I_9C"
   },
   "outputs": [],
   "source": [
    "def save_df_to_csv(df, filename):\n",
    "  df.to_csv(os.path.join(base_dir, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "01D6cjxAEzAZ"
   },
   "outputs": [],
   "source": [
    "def reader(corpus_path):\n",
    "    corpus_path, dirs, files = next(os.walk(corpus_path))\n",
    "    if \"license-blobs\" in dirs:\n",
    "        corpus_path = os.sep.join([corpus_path,\"license-blobs\"])\n",
    "        #otherwise assuming we're already in rcv1. \n",
    "        #just want to make the behavior robust here.\n",
    "        # you can pass either a Reuters corpus dir with RCV1 or RCV1 itself\n",
    "        # Don't care which.\n",
    "\n",
    "    corpus_path, dirs, files1 = next(os.walk(corpus_path))\n",
    "    for D1 in dirs:\n",
    "        result = []\n",
    "        dir_path = os.sep.join([corpus_path,D1])\n",
    "        dir_path, dirs2, files2 = next(os.walk(dir_path))\n",
    "        for D2 in dirs2:\n",
    "          dir_path2 = os.sep.join([dir_path, D2])\n",
    "          dir_path2, dirs3, files3 = next(os.walk(dir_path2))\n",
    "\n",
    "          for f in files3:\n",
    "            try:\n",
    "              data_path = os.sep.join([dir_path2, f])\n",
    "              with open(data_path, 'r', encoding=\"utf8\") as f3:\n",
    "                  raw_data = f3.read()\n",
    "              \n",
    "\n",
    "              def parse_text(raw_data): \n",
    "                  # HERE PUT STOPWORDS parsing\n",
    "                  return ' '.join([word for word in raw_data.split() if str(word).lower() not in (stop)])\n",
    "            \n",
    "              \n",
    "              #assemble output\n",
    "              output = {\n",
    "                        \"text\": parse_text(raw_data),\n",
    "                        \"path\": data_path\n",
    "                        }\n",
    "\n",
    "              result.append(output)\n",
    "            except Exception as e:\n",
    "              print(e)\n",
    "        print(f\"Saving {D1} results..\")\n",
    "        df = pd.DataFrame(result)\n",
    "        df.dropna(inplace=True,subset=['text'])\n",
    "        print(df.head())\n",
    "        if not os.path.exists(os.path.join(base_dir, 'dataset_parsed')):\n",
    "            os.makedirs(os.path.join(base_dir,'dataset_parsed'))\n",
    "        save_df_to_csv(df, f'dataset_parsed/{D1}_total_no_stopwords.csv')\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader(base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_telegram_message(text):\n",
    "    if TELEGRAM_BOT_TOKEN and TELEGRAM_CHAT_ID:\n",
    "        !pip install -q python-telegram-bot\n",
    "        from telegram import Bot\n",
    "        bot = Bot(token=TELEGRAM_BOT_TOKEN)\n",
    "        bot.send_message(text=text, chat_id=TELEGRAM_CHAT_ID)\n",
    "send_telegram_message(\"Finito di estrarre dataset completo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "file_num = 0\n",
    "words_set = set()\n",
    "cnt = Counter()\n",
    "def calculate_file_num(corpus_path):\n",
    "    global file_num\n",
    "    corpus_path, dirs, files = next(os.walk(corpus_path))\n",
    "    if \"dataset_parsed\" in dirs:\n",
    "        corpus_path = os.sep.join([corpus_path,\"dataset_parsed\"])\n",
    "    \n",
    "    for f in files:\n",
    "        print(f)\n",
    "        try:\n",
    "            data_path = os.sep.join([corpus_path, f])\n",
    "            document = pd.read_csv(data_path)\n",
    "            document.dropna(subset=['text'])\n",
    "            file_num += len(document['text'])\n",
    "            for index, row in document.iterrows():\n",
    "                # print(row['text'])\n",
    "                if isinstance(row['text'], str):\n",
    "                    for word in row['text'].split(' '):\n",
    "                        words_set.add(word)\n",
    "                        cnt[word] += 1\n",
    "            print(\"numero testi: {}\".format(len(document['text'])))\n",
    "            print(\"lunghezza set parole: {}\".format(len(words_set)))\n",
    "        except Exception as e:\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_file_num(\"dataset_parsed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"numero testi: {file_num}\")\n",
    "print(f\"lunghezza set parole: {len(words_set)}\")\n",
    "print(cnt.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cnt.most_common(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "send_telegram_message(f\"Finito di salvare stat dataset:\\nnumero testi: {file_num}\\nlunghezza set parole: {len(words_set)}\\n{cnt.most_common(10)}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Codice_estrazione.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
